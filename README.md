# LiveDB - Live Data System Simulation

A demonstration project that simulates how a live data system works, from data ingestion to event streaming and real-time processing.

## Project Purpose

This is a **simulation/demo module** designed to demonstrate the architecture and flow of a live data system. It shows how data flows from database insertion through event streaming to real-time processing and visualization.

## System Architecture

The complete live data system consists of four main components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Database       â”‚  Continuous data insertion
â”‚  (Neon Postgres)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka          â”‚  Event production and streaming
â”‚  (Event Bus)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Flink          â”‚  Real-time event processing
â”‚  (Processing)   â”‚  [To be implemented]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  UI Dashboard   â”‚  Live data visualization
â”‚  (Frontend)     â”‚  [To be implemented]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Current Implementation Status

### âœ… Implemented Modules

1. **Database Burst Writer** (`db-burst-writer/`)
   - âœ… Continuously inserts data into PostgreSQL database (2 rows every 60 seconds)
   - âœ… Reads seed data from CSV/JSON files
   - âœ… Cycles through seed data automatically
   - âœ… Simulates live data ingestion
   - âœ… Creates the data source for the system

2. **Kafka Producer** (`kafka/producer/`)
   - âœ… Reads data from PostgreSQL database
   - âœ… Polls database for new rows using offset tracking
   - âœ… Transforms database rows to Kafka events
   - âœ… Publishes events to Kafka topics
   - âœ… Continuous polling loop with configurable intervals
   - âœ… Replay mode support for reprocessing data

3. **Kafka Server** (`kafka/server/`)
   - âœ… Local Kafka infrastructure setup scripts
   - âœ… Automated startup script (ZooKeeper â†’ Kafka â†’ Topics)
   - âœ… ZooKeeper and Kafka broker management
   - âœ… Topic creation automation

### ğŸš§ Future Modules (Not Yet Implemented)

1. **Flink + Kafka Consumer**
   - Will consume events from Kafka topics
   - Process events in real-time
   - Transform and aggregate data streams

2. **UI Dashboard**
   - Visualize live data in real-time
   - Display processed results from Flink
   - Show system metrics and data flow

## Data Flow

1. **Data Ingestion**: Database Burst Writer continuously inserts data into PostgreSQL `livedb` table (2 rows every 60 seconds)
2. **Event Production**: Kafka Producer polls database for new rows and publishes events to Kafka `db_live_events` topic
3. **Event Processing**: Flink consumes events and processes them in real-time (future)
4. **Visualization**: UI Dashboard displays live data and processing results (future)

## Project Structure

```
livedb/
â”œâ”€â”€ db-burst-writer/          # Data ingestion module
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ index.js         # Main entry point
â”‚   â”‚   â”œâ”€â”€ db.js            # Database connection
â”‚   â”‚   â””â”€â”€ insert.js        # Insert operations
â”‚   â”œâ”€â”€ seed/
â”‚   â”‚   â””â”€â”€ livedb_seed.csv  # Seed data file
â”‚   â”œâ”€â”€ db_README.md         # Module documentation
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ kafka/                    # Event streaming module
â”‚   â”œâ”€â”€ producer/
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ index.js     # Producer entry point
â”‚   â”‚   â”‚   â”œâ”€â”€ producer.js  # Kafka producer logic
â”‚   â”‚   â”‚   â”œâ”€â”€ db.js        # Database client
â”‚   â”‚   â”‚   â””â”€â”€ mapper.js    # Row-to-event mapper
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â”œâ”€â”€ server/
â”‚   â”‚   â”œâ”€â”€ kafka-entry-point.bat  # Automated startup
â”‚   â”‚   â”œâ”€â”€ start-zookeeper.bat
â”‚   â”‚   â”œâ”€â”€ start-kafka-server.bat
â”‚   â”‚   â””â”€â”€ create-topics.bat
â”‚   â”œâ”€â”€ kafka-README.md      # Module documentation
â”‚   â””â”€â”€ SOP.md               # Detailed Kafka operations guide
â”‚
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ SOP.md                   # Kafka Standard Operating Procedure
```

## Quick Start Guide

### Prerequisites

- Node.js (v14 or higher)
- PostgreSQL database (local or cloud, e.g., Neon Postgres)
- Kafka installation (for local development)

### Setup Steps

1. **Set up Database Burst Writer:**
   ```bash
   cd db-burst-writer
   npm install
   # Create .env file with database credentials
   ```

2. **Set up Kafka Producer:**
   ```bash
   cd kafka/producer
   npm install
   # Create .env file with database and Kafka credentials
   ```

3. **Start Kafka Server** (if running locally):
   ```bash
   cd kafka/server
   .\kafka-entry-point.bat
   ```

4. **Run the Pipeline:**
   - Terminal 1: Start Database Burst Writer
     ```bash
     cd db-burst-writer
     npm start
     ```
   - Terminal 2: Start Kafka Producer
     ```bash
     cd kafka/producer
     npm start
     ```

## Use Cases

This simulation demonstrates:

- **Live Data Ingestion**: How continuous data insertion works
- **Event-Driven Architecture**: How database changes become events
- **Stream Processing**: How events flow through Kafka (Flink integration pending)
- **Real-Time Systems**: How data moves from source to visualization (UI pending)

## Documentation

- **Database Burst Writer**: See [`db-burst-writer/db_README.md`](./db-burst-writer/db_README.md)
- **Kafka Integration**: See [`kafka/kafka-README.md`](./kafka/kafka-README.md)
- **Kafka Operations Guide**: See [`SOP.md`](./SOP.md) for detailed Kafka troubleshooting and operations

## Database Schema

### `livedb` Table
```sql
CREATE TABLE "livedb" (
  "id" SERIAL PRIMARY KEY,
  "org" VARCHAR(255) NOT NULL,
  "amount" DECIMAL(10, 2) NOT NULL,
  "region" VARCHAR(255) NOT NULL,
  "created_at" TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### `producer_offsets` Table
```sql
CREATE TABLE "producer_offsets" (
  "id" INTEGER PRIMARY KEY DEFAULT 1,
  "last_id" INTEGER NOT NULL DEFAULT 0,
  "last_created_at" TIMESTAMP NOT NULL DEFAULT '1970-01-01',
  "updated_at" TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## Note

This project is part of a larger task to create a working simulation of a live data system. It demonstrates the foundational components (data ingestion and event production) with future components (Flink processing and UI dashboard) to be added.

---

**Task**: Create a working simulation which works in Live data from DB(neon)
